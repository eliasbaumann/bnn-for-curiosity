{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BNN_Curiosity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliasbaumann/bnn-for-curiosity/blob/master/BNN_Curiosity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DnE5E4GryQkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import threading\n",
        "import queue\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import mlagents\n",
        "# import obstacle_tower_env "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDjsTx5OzFkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### starting out by writing ppo\n",
        "## good code example here: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/discrete_DPPO.py\n",
        "## because its short and doesnt do anything extra\n",
        "## Next step would be to implement curiosity. But this should be the minimal baseline because thats already implemented in the Unity MLagents toolkit...<\n",
        "\n",
        "PATH = 'C:/Users/Elex/Downloads/obstacle-tower-challenge/ObstacleTower/obstacletower'\n",
        "\n",
        "OBS_DIM = 84*84*3\n",
        "ACTION_DIM = 54 # ALSO QUESTIONABLE IF THATS A GOOD IDEA -> LOOK AT WORK HOW TO DEAL WITH LARGE ACTION SPACE...\n",
        "CRITIC_LR = 0.0001\n",
        "ACTOR_LR = 0.0002\n",
        "EPSILON = .2\n",
        "GAMMA = .9\n",
        "\n",
        "EPISODE_MAX = 1000\n",
        "MIN_BATCH_SIZE = 64\n",
        "UPDATE_STEP=10 #?????\n",
        "\n",
        "NUMBER_OF_WORKERS = 4\n",
        "\n",
        "class PPO(object):\n",
        "  def _actor_network(self,name,trainable):\n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
        "      act = tf.layers.dense(self.inp,200,tf.nn.relu,trainable=trainable)\n",
        "      \n",
        "      \n",
        "      # mu = 2* tf.layers.dense(act,ACTION_DIM,tf.nn.tanh,trainable=trainable)\n",
        "      action_probs = tf.layers.dense(act,ACTION_DIM,tf.nn.softmax,trainable=trainable)\n",
        "      # norm_dist = tf.distributions.Normal(loc=mu,scale=sigma)\n",
        "    params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=name)\n",
        "    return action_probs,params\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.sess = tf.Session()\n",
        "    \n",
        "    self.inp = tf.placeholder(tf.float32,[None,OBS_DIM],name='state')\n",
        "    \n",
        "    # ppo is based on actor critic \n",
        "    \n",
        "    # Critic\n",
        "    crit = tf.layers.dense(self.inp,100,tf.nn.relu)\n",
        "    self.value = tf.layers.dense(crit,1)\n",
        "    self.dc_reward = tf.placeholder(tf.float32,[None,1],name='discounted_r')\n",
        "    self.advantage = self.dc_reward - self.value\n",
        "    self.critic_loss = tf.reduce_mean(tf.square(self.advantage))\n",
        "    self.critic_train_opt = tf.train.AdamOptimizer(CRITIC_LR).minimize(self.critic_loss)\n",
        "    \n",
        "    \n",
        "    # Actor \n",
        "    self.policy, theta_p = self._actor_network('policy',trainable=True)\n",
        "    old_policy, old_theta_p = self._actor_network('old_policy',trainable=False)\n",
        "    \n",
        "    self.update_old_policy_op =[oldp.assign(p) for p,oldp in zip(theta_p,old_theta_p)]\n",
        "    \n",
        "    self.action = tf.placeholder(tf.int32, [None,],name='action')\n",
        "    self.full_adv = tf.placeholder(tf.float32, [None,1],name='advantage')\n",
        "    \n",
        "    a_indices = tf.stack([tf.range(tf.shape(self.action)[0], dtype=tf.int32), self.action], axis=1)\n",
        "    pi_prob = tf.gather_nd(params=self.policy, indices=a_indices)   \n",
        "    oldpi_prob = tf.gather_nd(params=old_policy, indices=a_indices) \n",
        "    ratio = pi_prob/oldpi_prob\n",
        "    surr = ratio * self.full_adv\n",
        "    \n",
        "    \n",
        "    self.actor_loss = -tf.reduce_mean(tf.minimum(surr,tf.clip_by_value(ratio,1.-EPSILON,1.+EPSILON)*self.full_adv))\n",
        "    \n",
        "    self.actor_train_opt = tf.train.AdamOptimizer(ACTOR_LR).minimize(self.actor_loss)\n",
        "    \n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    \n",
        "  def get_action(self,state):\n",
        "    action_probs = self.sess.run(self.policy, {self.inp:state})\n",
        "    action = np.random.choice(range(action_probs.shape[1]),p=action_probs.ravel())\n",
        "    return action\n",
        "\n",
        "  def get_value(self,state):\n",
        "    return self.sess.run(self.value,{self.inp:state})[0,0] #TODO same as with get_action, need to figure out what comes out of here...\n",
        "\n",
        "  def update(self):\n",
        "    global GLOBAL_UPDATE_COUNTER\n",
        "    while not COORD.should_stop():\n",
        "      if GLOBAL_EPISODE < EPISODE_MAX:\n",
        "        UPDATE_EVENT.wait()\n",
        "        self.sess.run(self.update_old_policy_op)\n",
        "        data = [QUEUE.get() for _ in range(QUEUE.qsize())]\n",
        "        data = np.vstack(data)\n",
        "        state,action,reward = data[:, :OBS_DIM], data[:, OBS_DIM: OBS_DIM + 1].ravel(), data[:, -1:]\n",
        "        advantage = self.sess.run(self.advantage, {self.inp: state,self.dc_reward: reward})\n",
        "        # update actor and critic in a update loop\n",
        "        [self.sess.run(self.actor_train_opt, {self.inp: state, self.action: action, self.full_adv: advantage}) for _ in range(UPDATE_STEP)]\n",
        "        [self.sess.run(self.critic_train_opt, {self.inp: state, self.dc_reward: reward}) for _ in range(UPDATE_STEP)]\n",
        "        UPDATE_EVENT.clear()        # updating finished\n",
        "        GLOBAL_UPDATE_COUNTER = 0   # reset counter\n",
        "        ROLLING_EVENT.set()         # set roll-out available"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhgAxlDwzS8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Worker for multiple agents at the same time\n",
        "\n",
        "class Worker(object):\n",
        "  def __init__(self,wid):\n",
        "    self.wid = wid\n",
        "    self.env = obstacle_tower_env.ObstacleTowerEnv(PATH,retro=True,worker_id=wid)\n",
        "    self.ppo = GLOBAL_PPO\n",
        "  \n",
        "  def vectorize_state(self,state):\n",
        "    return None\n",
        "  \n",
        "  def work(self):\n",
        "    global GLOBAL_EPISODE,GLOBAL_RUNNING_REWARD,GLOBAL_UPDATE_COUNTER\n",
        "    while not COORD.should_stop():\n",
        "      state = self.env.reset()\n",
        "      state = np.expand_dims(state.flatten(), axis=0)\n",
        "      episode_reward = 0\n",
        "      done = False\n",
        "      buffer_state,buffer_action,buffer_reward = [],[],[]\n",
        "      while not done: \n",
        "        \n",
        "        if not ROLLING_EVENT.is_set():\n",
        "          ROLLING_EVENT.wait()\n",
        "          buffer_state,buffer_action,buffer_reward = [],[],[]\n",
        "        \n",
        "        # Add multiple runs until batch size\n",
        "        action = self.ppo.get_action(state)\n",
        "        state_,reward,done,_ = self.env.step(action)\n",
        "        state_ = np.expand_dims(state_.flatten(),axis=0)\n",
        "        \n",
        "        ### INSERT CURIOSITY HERE:\n",
        "        # reward += new_reward\n",
        "        \n",
        "        buffer_state.append(state)\n",
        "        buffer_action.append(action)\n",
        "        buffer_reward.append(reward) #he normalized this, probably smart to do when we add curiosity as well...\n",
        "        state = state_\n",
        "        episode_reward += reward\n",
        "        GLOBAL_UPDATE_COUNTER += 1\n",
        "        \n",
        "        # If enough state,action,reward triples are collected:\n",
        "        if GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE or done:\n",
        "          state_value = self.ppo.get_value(state_)\n",
        "          discounted_reward = []\n",
        "          for r in buffer_reward: #[::-1] he adds that, but need to check if needed\n",
        "            state_value = r + GAMMA * state_value\n",
        "            discounted_reward.append(state_value)\n",
        "          discounted_reward.reverse()\n",
        "          \n",
        "          bs, ba, br = np.vstack(buffer_state),np.vstack(buffer_action),np.vstack(discounted_reward) #[:,np.newaxis] ??\n",
        "          buffer_state,buffer_action,buffer_reward = [],[],[]\n",
        "          QUEUE.put(np.hstack((bs,ba,br)))\n",
        "          \n",
        "          ROLLING_EVENT.clear()\n",
        "          UPDATE_EVENT.set()\n",
        "          \n",
        "          if GLOBAL_EPISODE >= EPISODE_MAX:\n",
        "            COORD.request_stop()\n",
        "            break\n",
        "      \n",
        "      \n",
        "      if len(GLOBAL_RUNNING_REWARD) == 0: \n",
        "        GLOBAL_RUNNING_REWARD.append(episode_reward)\n",
        "      else:\n",
        "        GLOBAL_RUNNING_REWARD.append(GLOBAL_RUNNING_REWARD[-1]*0.9+episode_reward*0.1)\n",
        "      GLOBAL_EPISODE += 1\n",
        "      print('{0:.1f}%'.format(GLOBAL_EPISODE/EPISODE_MAX*100), '|W%i' % self.wid,  '|Ep_r: %.2f' % episode_reward,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ny4WEnczkEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### MAIN, in the actual implementation uncomment the next line and re-indent everything\n",
        "# if __name__=='__main__':\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "GLOBAL_PPO = PPO()\n",
        "UPDATE_EVENT, ROLLING_EVENT = threading.Event(), threading.Event()\n",
        "UPDATE_EVENT.clear()\n",
        "ROLLING_EVENT.set()\n",
        "workers = [Worker(wid=i) for i in range(NUMBER_OF_WORKERS)]\n",
        "\n",
        "GLOBAL_UPDATE_COUNTER = 0\n",
        "GLOBAL_EPISODE = 0\n",
        "\n",
        "GLOBAL_RUNNING_REWARD = []\n",
        "\n",
        "COORD = tf.train.Coordinator()\n",
        "QUEUE = queue.Queue()\n",
        "threads = []\n",
        "for worker in workers:\n",
        "  thread = threading.Thread(target=worker.work,args=())\n",
        "  thread.start()\n",
        "  threads.append(thread)\n",
        "ppo_update_thread = threading.Thread(target=GLOBAL_PPO.update)\n",
        "ppo_update_thread.start()\n",
        "threads.append(ppo_update_thread)\n",
        "COORD.join(threads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxVyTgLMz5Oj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO, calculate additional reward from curiosity \n",
        "STATE_SHAPE = [None,84,84,3]\n",
        "STATE_LATENT_SHAPE = 64\n",
        "\n",
        "\n",
        "class Curiosity(object):\n",
        "  def __init__(self,sess,s_t,s_t1,a_t):\n",
        "    self.sess = sess\n",
        "    self.s_t = s_t\n",
        "    self.s_t1 = s_t1\n",
        "    self.a_t = a_t\n",
        "    \n",
        "    \n",
        "    self.inp_st = tf.placeholder(tf.float32,STATE_SHAPE,name='S_t_input')\n",
        "    # self.inp_st1 = tf.placeholder(tf.float32,STATE_SHAPE,name='S_t+1_input')\n",
        "    self.inp_at = tf.placeholder(tf.float32,[None,],name = 'A_t_input')\n",
        "    \n",
        "  \n",
        "  def cnn_feature_extractor(self):\n",
        "    cn1 = tf.layers.conv2d()\n",
        "  \n",
        "  def forward_model(self):\n",
        "    l1 = tf.layers.dense(tf.concat(),200,tf.nn.relu,trainable=trainable)\n",
        "    \n",
        "  def reverse_model(self):\n",
        "    \n",
        "  \n",
        "  def get_reward(self):\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}